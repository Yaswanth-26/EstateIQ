name: Model Monitoring

on:
  schedule:
    - cron: '0 0 * * 0'  # Run every Sunday at midnight
  workflow_dispatch:      # Allow manual triggering

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
  PROJECT_ID: estateiqclone
  REGION: us-central1
  ARTIFACT_REGISTRY: estateiq-models
  MODEL_PATH: models/estate_price_prediction
  SA_EMAIL: self@estateiqclone.iam.gserviceaccount.com
  GMAIL_USER: ${{ secrets.GMAIL_USER }}
  GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
  NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
  MLFLOW_TRACKING_URI: sqlite:///mlflow.db

jobs:
  monitor:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install numpy pandas scikit-learn supabase scipy ydata-profiling dvc dvc[gs] mlflow google-cloud-storage fairlearn xgboost

    - name: Google Auth
      uses: google-github-actions/auth@v2.1.2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2.1.0

    - name: Create monitoring script
      run: |
        cat > monitor.py << 'EOL'
        import os
        import pandas as pd
        import numpy as np
        from scipy import stats
        from supabase import create_client
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        import joblib
        from ydata_profiling import ProfileReport
        
        # Initialize Supabase client
        supabase = create_client(os.environ['SUPABASE_URL'], os.environ['SUPABASE_SERVICE_KEY'])
        
        # Fetch recent feedback data
        def fetch_feedback_data():
            response = supabase.table('feedback') \
                .select("*") \
                .order('created_at', desc=True) \
                .limit(100) \
                .execute()
            return pd.DataFrame(response.data)
        
        # Load production model
        model = joblib.load('model.joblib')
        
        # Fetch test data used in training
        def fetch_test_data():
            df = pd.read_csv('Data/final/X_test.csv')
            
            # Log test data info
            print("Test data loaded from Data/final/X_test.csv")
            print(f"Shape: {df.shape}")
            print("\nFeatures:")
            print(df.columns.tolist())
            print("\nSample data:")
            print(df.head())
            
            return df
        
        # Calculate model performance metrics
        def calculate_metrics(y_true, y_pred):
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mae = mean_absolute_error(y_true, y_pred)
            return rmse, mae
        
        # Detect data drift using KS test
        def detect_data_drift(reference_data, current_data, threshold=0.05):
            drift_detected = False
            drift_features = []
            
            for column in reference_data.columns:
                if reference_data[column].dtype in ['int64', 'float64']:
                    statistic, p_value = stats.ks_2samp(
                        reference_data[column],
                        current_data[column]
                    )
                    if p_value < threshold:
                        drift_detected = True
                        drift_features.append(column)
            
            return drift_detected, drift_features
        
        def main():
            # Fetch recent feedback
            feedback_df = fetch_feedback_data()
            
            # Prepare actual vs predicted values
            y_true = feedback_df['actual_price'].values
            y_pred = feedback_df['predicted_price'].values
            
            # Calculate metrics on feedback data
            rmse, mae = calculate_metrics(y_true, y_pred)
            
            # Load test data used in training
            test_data = fetch_test_data()
            
            # Extract features from property_data JSON
            feedback_features = pd.json_normalize(feedback_df['property_data'])
            
            # Base numeric features (exact names from training)
            numeric_features = [
                'GROSS_AREA', 'LIVING_AREA', 'LAND_SF', 'YR_BUILT',
                'BED_RMS', 'FULL_BTH', 'HLF_BTH', 'NUM_PARKING',
                'FIREPLACES', 'KITCHENS', 'TT_RMS', 'ZIP_CODE',
                'YR_REMODEL'
            ]
            
            # Convert numeric columns
            for col in numeric_features:
                if col in test_data.columns and col in feedback_features.columns:
                    try:
                        test_data[col] = pd.to_numeric(test_data[col], errors='coerce')
                        feedback_features[col] = pd.to_numeric(feedback_features[col], errors='coerce')
                        print(f"✓ Successfully converted {col}")
                    except Exception as e:
                        print(f"✗ Failed to convert {col}: {str(e)}")
            
            # Add derived features (same as training)
            print("\nAdding derived features...")
            for df in [test_data, feedback_features]:
                # Property age and renovations
                df['property_age'] = 2025 - df['YR_BUILT'].astype(int)
                if 'YR_REMODEL' in df.columns:
                    df['years_since_renovation'] = 2025 - df['YR_REMODEL'].fillna(df['YR_BUILT']).astype(int)
                    df['has_renovation'] = (df['YR_REMODEL'] > df['YR_BUILT']).astype(int)
                
                # Area ratios
                df['living_area_ratio'] = np.where(df['GROSS_AREA'] > 0, df['LIVING_AREA'] / df['GROSS_AREA'], 0)
                df['floor_area_ratio'] = np.where(df['LAND_SF'] > 0, df['GROSS_AREA'] / df['LAND_SF'], 0)
                df['non_living_area'] = np.maximum(0, df['GROSS_AREA'] - df['LIVING_AREA'])
                df['rooms_per_area'] = np.where(df['LIVING_AREA'] > 0, df['TT_RMS'] / df['LIVING_AREA'], 0)
                
                # Bathroom calculations
                df['total_bathrooms'] = df['FULL_BTH'] + 0.5 * df['HLF_BTH']

                # Calculate condition scores
                condition_map = {'E': 5, 'VG': 4.5, 'G': 4, 'A': 3, 'F': 2, 'P': 1}
                
                def calculate_condition_score(row, condition_cols):
                    max_score = 3  # default score
                    for col in condition_cols:
                        if row[col]:  # if this condition is true
                            cond_code = col.split(' - ')[0].split('_')[-1]
                            score = condition_map.get(cond_code, 0)
                            max_score = max(max_score, score)
                    return max_score
                
                # Interior score
                int_cond_cols = [col for col in df.columns if col.startswith('INT_COND_')]
                if int_cond_cols:
                    df['interior_score'] = df.apply(
                        lambda row: calculate_condition_score(row, int_cond_cols), axis=1
                    )
                
                # Exterior score
                ext_cond_cols = [col for col in df.columns if col.startswith('EXT_COND_')]
                if ext_cond_cols:
                    df['exterior_score'] = df.apply(
                        lambda row: calculate_condition_score(row, ext_cond_cols), axis=1
                    )
                
                # Overall score
                overall_cond_cols = [col for col in df.columns if col.startswith('OVERALL_COND_')]
                if overall_cond_cols:
                    df['overall_score'] = df.apply(
                        lambda row: calculate_condition_score(row, overall_cond_cols), axis=1
                    )
            
            print("\nChecking for drift using processed features...")
            processed_features = numeric_features + [
                'property_age', 'years_since_renovation', 'has_renovation',
                'living_area_ratio', 'floor_area_ratio', 'non_living_area',
                'rooms_per_area', 'total_bathrooms',
                'interior_score', 'exterior_score', 'overall_score'
            ]
            
            # Filter for features present in both datasets
            drift_features = [f for f in processed_features if f in test_data.columns and f in feedback_features.columns]
            print(f"\nUsing {len(drift_features)} features for drift detection:")
            print(drift_features)

            # Check for data drift using processed features
            available_features = [f for f in processed_features if f in test_data.columns and f in feedback_features.columns]
            drift_detected, drift_features = detect_data_drift(
                test_data[available_features],
                feedback_features[available_features]
            )
            
            # Generate data profile report
            profile = ProfileReport(
                pd.json_normalize(feedback_df['property_data']),
                title="Feedback Data Profile"
            )
            profile.to_file("feedback_profile.html")
            
            # Determine if retraining is needed
            retraining_needed = (
                drift_detected or
                rmse > 150000 or  # Threshold from monitoring config
                mae > 100000      # Threshold from monitoring config
            )
            
            # Write results
            with open('monitoring_results.txt', 'w') as f:
                f.write(f"RMSE: {rmse}\n")
                f.write(f"MAE: {mae}\n")
                f.write(f"Data Drift Detected: {drift_detected}\n")
                if drift_features:
                    f.write(f"Drift Features: {', '.join(drift_features)}\n")
                f.write(f"Retraining Needed: {retraining_needed}")
            
            # Exit with status code for workflow
            exit(0 if not retraining_needed else 1)
        
        if __name__ == "__main__":
            main()
        EOL

    - name: Setup DVC and Pull Data
      env:
        GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_SA_KEY }}
      run: |
        echo '${{ secrets.GCP_SA_KEY }}' > key.json
        
        # Configure DVC remotes
        dvc remote modify data credentialpath key.json
        
        # Debug - List remotes
        echo "DVC Remotes:"
        dvc remote list
        
        # Pull data
        dvc pull || echo "Warning: DVC pull had issues"
        
        # Create Data directory if it doesn't exist
        mkdir -p Data
        
        # Create symbolic links from data to Data
        if [ -d "data" ]; then
          echo "Creating symbolic links from data/ to Data/"
          for dir in data/*; do
            if [ -d "$dir" ]; then
              base=$(basename "$dir")
              mkdir -p "Data/$base"
              cp -r "$dir"/* "Data/$base/"
            fi
          done
        else
          echo "Warning: data directory not found after DVC pull"
        fi
        
        # Debug - List directories
        echo "Directory structure after setup:"
        ls -la
        echo "Data directory contents:"
        ls -la Data/ || echo "No Data directory"
        echo "data directory contents:"
        ls -la data/ || echo "No data directory"

    - name: Download current model
      run: |
        gcloud storage cp gs://${{ env.ARTIFACT_REGISTRY }}/${{ env.MODEL_PATH }}/current/model.joblib .

    - name: Run monitoring
      id: monitoring
      continue-on-error: true
      run: python monitor.py

    - name: Upload monitoring results
      uses: actions/upload-artifact@v4
      with:
        name: monitoring-results
        path: |
          monitoring_results.txt
          feedback_profile.html
        retention-days: 30

    - name: Train Model if needed
      if: steps.monitoring.outcome == 'failure'
      run: |
        gcloud builds submit . \
          --config=cloudbuild.yaml \
          --project=${{ env.PROJECT_ID }} \
          --substitutions=_PROJECT_ID=${{ env.PROJECT_ID }},_ARTIFACT_REGISTRY=${{ env.ARTIFACT_REGISTRY }},_MODEL_PATH=${{ env.MODEL_PATH }},_REGION=${{ env.REGION }},_MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }},_GMAIL_USER="${{ env.GMAIL_USER }}",_GMAIL_APP_PASSWORD="${{ env.GMAIL_APP_PASSWORD }}",_NOTIFICATION_EMAIL="${{ env.NOTIFICATION_EMAIL }}"

    - name: Verify Training Success
      if: steps.monitoring.outcome == 'failure'
      id: verify_training
      run: |
        # Wait for metrics file
        for i in {1..10}; do
          if gcloud storage ls gs://${{ env.ARTIFACT_REGISTRY }}/${{ env.MODEL_PATH }}/current/metrics.json; then
            echo "Metrics file found"
            break
          fi
          if [ $i -eq 10 ]; then
            echo "Timeout waiting for metrics file"
            exit 1
          fi
          sleep 30
        done

        # Check metrics
        gcloud storage cp gs://${{ env.ARTIFACT_REGISTRY }}/${{ env.MODEL_PATH }}/current/metrics.json ./metrics.json
        if jq -e '.metrics.r2' metrics.json > /dev/null; then
          echo "::set-output name=training_success::true"
        else
          echo "::set-output name=training_success::false"
          exit 1
        fi

    - name: Notify on Failure
      if: failure()
      run: |
        echo "Model monitoring/training pipeline failed. Please check the logs for details."
        exit 1

    - name: Notify on completion
      if: always()
      run: |
        if [ -f monitoring_results.txt ]; then
          echo "Model Monitoring Results:"
          cat monitoring_results.txt
        fi
